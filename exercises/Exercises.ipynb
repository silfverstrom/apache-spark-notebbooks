{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Exercises\n",
    "\n",
    "This is a collection of PySpark exercises.\n",
    "The solution to all of these problems can be found in the solutions folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To start solving problems with pyspark, we first need to import it\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 First Exercise\n",
    "\n",
    "This is our first test of PySpark. In this simple exercise we are going to create a RDD from a list in python.\n",
    "We are then going to apply a transformer, map, and tripple the values in the RDD.\n",
    "\n",
    "We will then use a action, sum, that returns the total sum of all values.\n",
    "We will then assert that the total sum is 15150.\n",
    "\n",
    "Spark functions you will need to use,\n",
    "\n",
    "* sc.parallelize # To convert list to RDD\n",
    "* rdd.map - to transform data\n",
    "* sum - to get total sum\n",
    "\n",
    "\n",
    "In pseudo code:\n",
    "\n",
    "data = list_of_100_numbers()\n",
    "\n",
    "rdd = sc.parallelize(data) # Convert list to rdd\n",
    "rdd = rdd.map(lambda_function_that_tripples_number)\n",
    "result = rdd.sum() # Sum all our values\n",
    "assert(result == 15150)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Read a csv and split the columns\n",
    "\n",
    "In this exercise we will read a csv from s3.\n",
    "We will then use a transformer, flatMap, that splits the columns for us.\n",
    "\n",
    "We will then collect all results to see that they are parsed correctly.\n",
    "\n",
    "The path to the data is:\n",
    "\n",
    "s3://link-workshops/example_data.csv\n",
    "\n",
    "\n",
    "Spark functions you will need to use,\n",
    "\n",
    "* sc.textFile(path) # To get RDD from csv\n",
    "* rdd.flatMap - to transform data to cols\n",
    "* rdd.collect - To collect results\n",
    "\n",
    "\n",
    "In pseudo code:\n",
    "\n",
    "rdd = sc.textFile(path)\n",
    "\n",
    "rdd = rdd.flatMap(function_that_splits_csv)\n",
    "rddd.collect()\n",
    "\n",
    "Output should look something like:\n",
    "['1', '200', '2', '500', '3', '-99', '4', '99', '5', '600']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 # Read a csv from s3. Find users with less score than zero, and return their ids.\n",
    "\n",
    "\n",
    "In this exercise we will use our previously loaded csv-file.\n",
    "Our csv-file has the following structure:\n",
    "    0: user_id\n",
    "    1: score\n",
    "        \n",
    "We will then find all rows that have a score below zero, and return their id.\n",
    "\n",
    "We will then collect all results.\n",
    "\n",
    "\n",
    "Spark functions you will need to use,\n",
    "\n",
    "* sc.map(path) - To transform data\n",
    "* rdd.filter - to filter data\n",
    "* rdd.collect - To collect results\n",
    "\n",
    "\n",
    "In pseudo code:\n",
    "\n",
    "rdd = rdd.map(parseRow).filter(filterLessThanZero).map(getId)\n",
    "less_than_zero = rdd.collect()\n",
    "\n",
    "Output should look something like:\n",
    "[3]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.4 Take a list of values, and translate it through a lookup table.\n",
    "\n",
    "In this exercise we will take a list of values, and translate it through a lookup table.\n",
    "But to save time, broadcast a read only variable first.\n",
    "\n",
    "We will then find all rows that have a score below zero, and return their id.\n",
    "\n",
    "We will then collect all results.\n",
    "\n",
    "\n",
    "Spark functions you will need to use,\n",
    "\n",
    "* sc.spark.sparkContext.broadcast(variabble) - To broadcast data between nodes\n",
    "* spark.sparkContext.parallelize - create rdd from list\n",
    "* rdd.map - To convert results\n",
    "* rdd.collect - To collect results\n",
    "\n",
    "\n",
    "The setup should look like:\n",
    "\n",
    "countries  = {\"sv\": \"sweden\", \"us\": \"usa\"}\n",
    "data = [\"sv\",\"us\"]\n",
    "broadcast_countries = spark.sparkContext.broadcast(countries)\n",
    "\n",
    "parallelize, map and collect\n",
    "\n",
    "Output should look something like:\n",
    "['sweden', 'usa']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Create an accumulator and implement a custom sum operation.\n",
    "\n",
    "\n",
    "Read our previously loaded csv-file.\n",
    "Then sum the scores of the example_data.csv\n",
    "\n",
    "\n",
    "Spark functions you will need to use,\n",
    "\n",
    "* sc.accumulator(0) - to create accumulator\n",
    "* accum.add - to add values to accumulator\n",
    "* accum.value - to get accumulator values\n",
    "* rdd.map - To convert results\n",
    "* rdd.foreach - To to iterate over results\n",
    "\n",
    "The setup should look like:\n",
    "\n",
    "accum=sc.accumulator(0)\n",
    "read_file()\n",
    "custom_summary()\n",
    "assert(accum.value == 1300)\n",
    "\n",
    "Output should be\n",
    "1300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
